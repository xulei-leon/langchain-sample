{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver, InMemorySaver\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API key get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# key\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "silicon_api_key = os.getenv(\"SILICON_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# deepseek\n",
    "deepseek_llm_model = \"deepseek-chat\"\n",
    "\n",
    "# silicon\n",
    "silicon_base_url =  \"https://api.siliconflow.cn/v1\"\n",
    "silicon_llm_model = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# huggingface\n",
    "huggingface_embed_model = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init LLM mod\n",
    "llm_silicon = ChatOpenAI(\n",
    "    model=silicon_llm_model,\n",
    "    openai_api_key=silicon_api_key,\n",
    "    base_url=silicon_base_url,\n",
    "    temperature=0.8,\n",
    "    max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# init LLM mod\n",
    "llm_deepseek = ChatDeepSeek(\n",
    "    model=deepseek_llm_model,\n",
    "    temperature=0.3,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.7,\n",
    "    presence_penalty=0.5,\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Re-writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### Deepseek Question Re-writer\n",
    "################################################################################\n",
    "def rewriter_question(llm: ChatDeepSeek, question: str)->str:\n",
    "    system_role = \"\"\"\n",
    "    Role: Question Optimization Specialist\n",
    "    Task: Re-write input question to a better version\n",
    "\n",
    "    Guidelines:\n",
    "    You are a question re-writer that converts an input question to a better version that is optimized \\\n",
    "for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "    \"\"\"\n",
    "\n",
    "    system_respond = \"\"\"\n",
    "    Respond to the request:\n",
    "    1. direct response to core content\n",
    "    2. disable examples/extended descriptions\n",
    "    3. use simple sentence structure\n",
    "    4. Omit non-critical details\n",
    "    \n",
    "    Current Scenario: Rapid Response Mode\n",
    "    \"\"\"\n",
    "\n",
    "    key = \"refined_question\"\n",
    "    system_format = f\"\"\"\n",
    "    **Format Requirement**:\n",
    "    - Respond ONLY with a JSON object\n",
    "    - Key: '{key}'\n",
    "    - Value: The improved question\n",
    "    - No explanations or additional text\n",
    "    \"\"\"\n",
    "\n",
    "    instruction = \"Please refine and improve this question.\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f\"{system_role}\\n{system_respond}\\n{system_format}\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"Original question:\\n {{question}}\\n\\n {instruction}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #print(\"=== deepseek question prompt template ===\\n\")\n",
    "    #prompt.pretty_print()\n",
    "\n",
    "\n",
    "    parser = JsonOutputParser()\n",
    "    rewriter = prompt | llm | parser\n",
    "\n",
    "    try:\n",
    "        doc = rewriter.invoke({\"question\": question})\n",
    "        rewrite = doc[key]\n",
    "    except KeyError:\n",
    "        logging.error(\"KeyError: 'refined_question' not found in response\")\n",
    "        rewrite = question\n",
    "    \n",
    "    return rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Test Question Re-writer\n",
    "##\n",
    "\n",
    "user_input = \"What are the ingredients in Alpha Hope?\"\n",
    "\n",
    "rewrite_question = rewriter_question(llm=llm_deepseek, question=user_input)\n",
    "\n",
    "print(\"\\n=== re-write question ===\\n\")\n",
    "print(rewrite_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval documents re-writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### Deepseek Retrieval documents re-writer\n",
    "################################################################################\n",
    "def rewriter_document(llm: ChatDeepSeek, question: str, document: str)->str:\n",
    "    #\n",
    "    # role\n",
    "    #\n",
    "    role = \"\"\"\n",
    "        \\rRole: Documents Optimization Specialist\n",
    "        \\rTask: re-write input document to a better version\n",
    "        \\r\n",
    "        \\rGuidelines:\n",
    "        \\rYou are a re-writer that converts a retrieved document to a better version that is optimized.\n",
    "        \\rRewrite the part of the document that relates to the user question and output them.\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # respond request\n",
    "    #\n",
    "    respond_request = \"\"\"\n",
    "        \\rRespond to the request:\n",
    "        \\r1. direct response to core content\n",
    "        \\r2. disable examples/extended descriptions\n",
    "        \\r3. use simple sentence structure\n",
    "        \\r4. Omit non-critical details\n",
    "        \\r\n",
    "        \\rCurrent Scenario: Rapid Response Mode\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # response format\n",
    "    #\n",
    "    json_parser = JsonOutputParser()\n",
    "\n",
    "\n",
    "    response_format_template = PromptTemplate(\n",
    "        template=\"\\\n",
    "            \\r**Format Requirement**:\\n\\\n",
    "            \\r- {format_instructions}\\n\\\n",
    "            \\r- Key: {json_key}\\n\\\n",
    "            \\r- Value: The improved document\\n\\\n",
    "            \\r- No explanations or additional text\",\n",
    "        input_variables=[\"json_key\"],\n",
    "        partial_variables={\"format_instructions\": \"Return a JSON object.\"},\n",
    "    )\n",
    "\n",
    "    key = \"refined_question\"\n",
    "    response_format = response_format_template.format(json_key=key)\n",
    "\n",
    "    #\n",
    "    # response format\n",
    "    #\n",
    "    rewriter_instruction = \"Please refine and improve this document.\"\n",
    "\n",
    "    #\n",
    "    # chat prompt\n",
    "    #\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f\"{role}\\n{respond_request}\\n{response_format}\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"User question:\\n {{question}}\\n\\n Original retrieved document:\\n\\n {{document}}\\n\\n {rewriter_instruction}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"=== deepseek question prompt template ===\\n\")\n",
    "    prompt.pretty_print()\n",
    "\n",
    "    rewriter = prompt | llm | json_parser\n",
    "    try:\n",
    "        doc = rewriter.invoke({\"question\": question, \"document\": document})\n",
    "        rewrite = doc[key]\n",
    "    except KeyError:\n",
    "        logging.error(\"KeyError: 'refined_question' not found in response\")\n",
    "        rewrite = document\n",
    "    \n",
    "    return rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Test Document Re-writer\n",
    "##\n",
    "\n",
    "user_input = \"What are the ingredients in Alpha Hope?\"\n",
    "retrieval_document = \"\"\"\n",
    "Alpha Hope has been formulated with two powerful active ingredients, PQQ and Molecular Hydrogen.\n",
    "They work synergistically to activate metabolic pathways involved in energy production and cognition.\n",
    "This is particularly formulated to promote the bodyâ\\x80\\x99s natural detox process and help the body naturally produce Hope Molecules,\\\n",
    "also known as PGC-1Î±, that fight oxidative damage.\n",
    "\"\"\"\n",
    "\n",
    "rewrite_document = rewriter_document(llm=llm_deepseek, question=user_input, document=retrieval_document)\n",
    "\n",
    "print(\"\\n=== re-write document ===\\n\")\n",
    "print(rewrite_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "template = \"\"\"\n",
    "You are an AI assistant. Your task is to help the user with their queries.\n",
    "\n",
    "User: {user_input}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "user_input = \"What is the capital of France?\"\n",
    "prompt = prompt_template.format(user_input=user_input)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "#parser = JsonOutputParser(pydantic_object=Joke)\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "print(\"\\n=== format instructions ===\\n\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "user_input = \"What is the capital of France?\"\n",
    "prompt = prompt_template.format(query=user_input)\n",
    "print(\"\\n=== prompt ===\\n\")\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
